{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Cohort of Songs\n",
    "Steps in exact order\n",
    "1. Perform data cleansing for removing outliers, missing values, duplicates (`drop_duplicates()`)\n",
    "2. Perform data preprocessing:\n",
    "    - More feature engineering\n",
    "    - encoding\n",
    "    - ignore scaling so you can perform EDA first\n",
    "    - convert release date to datetime and extract date parts. Hint: use `to_datetime()`, then for example extract year part using `df['release_date'].dt.year`  \n",
    "3. EDA:\n",
    "    - We can take the average popularity score for each album and use `nlargest()`\n",
    "    - Conduct exploratory data analysis to delve into various features of songs, aiming to identify patterns\n",
    "        - year on x-axis and numeric values (e.g. energy) on y-axis and observe the pattern/trend for each (line graph)\n",
    "        - check for association between numeric features:\n",
    "            - correlation matrix (linear analysis)\n",
    "            - scatter/regplot for numeric columns e.g. popularity vs acousticness (linear or non-linear)\n",
    "        - Examine the relationship between a song's popularity and various factors, exploring how this correlation has evolved:\n",
    "            - create a new column for decade\n",
    "            - build a popularity vs acousticness plot for each decade (do the same for the rest of the numeric values)\n",
    "        - Provide insights on the significance of dimensionality reduction techniques. Share your ideas and elucidate your observations:\n",
    "            - Perform analysis of variance for PCA and suggest the appropriate number of PCs to get a 90 to 95 cumsum variance\n",
    "4. Cluster analysis:\n",
    "    - Use Kmeans and the elbow method to identify the proper number of clusters\n",
    "    - After building the clusters, perform the mean, median, etc... analysis on every numeric feature by cluster.\n",
    "    - Visualize the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employee Turnover Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Same standard DQA\n",
    "2. EDA\n",
    "    - Build a heatmap for numeric columns\n",
    "    - Check if the target is imbalanced using bar plots\n",
    "    - Build histograms for all 3 columns\n",
    "    - Use `countplot()` with left as hue\n",
    "3. Build the clusters \n",
    "    - based on 2 features (Satisfaction and last evaluation) with filter left =1\n",
    "    - use Kmeans with 3 clusters\n",
    "    - An example of a cluster you may observer: some employees have high evaluation score (high performers), but left the company and weren't satisfied\n",
    "4. Working with imbalanced data\n",
    "    - Preprocessing using encoding and standardization\n",
    "    - no need to separate. you can perform `get_dummies()` on the whole dataframe, just specify the categorical columns\n",
    "    - Use `SMOTE()` for imbalanced data\n",
    "    - In `train_test_split()` use `stratify=y` **note how we changed the order compared to the problem statement as it makes more snense to split after SMOTE()**\n",
    "5. Model Building:\n",
    "    - Build all 3 models with 5-fold cross-validation\n",
    "    - For each model, run `sklearn.metrics.classification_report(y_true, y_pred)`\n",
    "6. Build AUC/ROC curves (for each model)\n",
    "    - Build ROC/AUC for each mode\n",
    "    - Plot all 3 curves and compare\n",
    "7. Steps:\n",
    "    - Pick the best model. For example, for lineargression, suse `LR_model.predict_proba(X_test)` to calculate the probabilities for X_test\n",
    "    - Using the probabilities, break down the outcome into 4 groups:\n",
    "        - Safe Zone (Green) (Score < 20%)\n",
    "        - Low Risk Zone (Yellow) (20% < Score < 60%)\n",
    "        - Medium Risk Zone (Orange) (60% < Score < 90%)\n",
    "        - High Risk Zone (Red) (Score > 90%)\n",
    "    - Build all the probabilities as a column and create another column based on the conditions above\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
